from llama_cpp import Llama
from dotenv import load_dotenv
import os
import time

load_dotenv()

class ChatPhi():
    
    def __init__(self):
        self.llm = Llama(
                model_path=os.getenv("model_path"),
                n_ctx=256,
                n_threads=12,
                n_batch=1024,
                use_mmap=True,
                use_mlock=True, 
            )
        
    def get_response(self, query : str):
        for chunk in self.llm(
            f"<|user|>\n{query}<|end|>\n<|assistant|>", 
            max_tokens=100, 
            temperature=0, 
            stream=True
            ):
            print(chunk["choices"][0]["text"], end="", flush=True)
    
    def get_response_summary(self, query : str, response : str):
        query = f"Summarise the  question - asked by user and answer - provided by a system bot, pair provided in less than 150 words. question : {query} , answer : {response}"
        start = time.time()
        response = self.llm(
            f"<|user|>\n{query}<|end|>\n<|assistant|>", 
            max_tokens=50, 
            temperature=0,
             
        )
        end = time.time()
        diff = end-start
        print(response)
        print("tinme taken : ", diff)
        #print(response["choices"][0]["message"]["content"])

model = ChatPhi()
model.get_response_summary(query="How to run fastapi", response="To run FastAPI, first install FastAPI and Uvicorn using pip. Then, create a Python file with a FastAPI application. Inside it, define a basic route that returns a message. After that, start the server using the uvicorn command by specifying the filename and app instance. Add the --reload option to enable auto-reloading during development. Once the server is running, open a browser and go to http://127.0.0.1:8000 to see the response. You can also visit http://127.0.0.1:8000/docs to access the interactive API documentation automatically generated by FastAPI.")